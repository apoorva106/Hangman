{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "from urllib.parse import parse_qs\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        self.verify_cuda_usage()\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "\n",
    "        # Suppress SSL warnings\n",
    "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        sample_words = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n",
    "\n",
    "        # Build the dictionary and sample words\n",
    "        dictionary_file_location = \"words_250000_train.txt\"\n",
    "        #self.full_dictionary = self.build_dictionary(dictionary_file_location)\n",
    "        self.full_dictionary = sample_words\n",
    "        self.df_aug = self.create_intermediate_data(self.full_dictionary)\n",
    "    \n",
    "        # Create masked dictionary and vowel prior\n",
    "        self.masked_dictionary = self.create_masked_dictionary(self.df_aug)\n",
    "        self.vowel_prior = self.get_vowel_prior(self.df_aug)\n",
    "        self.save_vowel_prior(self.vowel_prior)\n",
    "\n",
    "        # Encode data\n",
    "        self.input_data, self.target_data = self.encode_words(self.masked_dictionary)\n",
    "        self.save_input_output_data(self.input_data, self.target_data)\n",
    "        self.input_tensor, self.target_tensor = self.convert_to_tensor(self.input_data, self.target_data)\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.train_model(epochs=8, lr=0.01, batch_size=128)  # Adjust epochs and lr as needed\n",
    "\n",
    "\n",
    "    def verify_cuda_usage(self):\n",
    "        if self.device.type == 'cuda':\n",
    "            print('CUDA is being used.')\n",
    "        else:\n",
    "            print('CUDA is not being used.')\n",
    "\n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "            requests.get(link)\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def create_intermediate_data(self, words):\n",
    "        df = pd.DataFrame(words, columns=['word'])\n",
    "        df['length'] = df['word'].apply(len)\n",
    "        df['vowels_present'] = df['word'].apply(lambda p: set(p).intersection({'a', 'e', 'i', 'o', 'u'}))\n",
    "        df['vowels_count'] = df['vowels_present'].apply(len)\n",
    "        df['unique_char_count'] = df['word'].apply(lambda p: len(set(p)))\n",
    "        df = df[~((df['unique_char_count'].isin([0, 1, 2])) | (df['length'] <= 3)) & (df['vowels_count'] != 0)]\n",
    "        return df\n",
    "\n",
    "    def loop_for_permutation(self, unique_letters, word, all_perm, i):\n",
    "        random_letters = random.sample(unique_letters, i + 1)\n",
    "        new_permuted_word = word\n",
    "        for letter in random_letters:\n",
    "            new_permuted_word = new_permuted_word.replace(letter, \"_\")\n",
    "            all_perm.append(new_permuted_word)\n",
    "\n",
    "    def permute_all(self, word, vowel_permutation_loop=False):\n",
    "        unique_letters = list(set(word))\n",
    "        all_perm = []\n",
    "        if vowel_permutation_loop:\n",
    "            for i in range(len(unique_letters) - 1):\n",
    "                self.loop_for_permutation(unique_letters, word, all_perm, i)\n",
    "            all_perm = list(set(all_perm))\n",
    "            return all_perm\n",
    "        else:\n",
    "            for i in range(len(unique_letters) - 2):\n",
    "                self.loop_for_permutation(unique_letters, word, all_perm, i)\n",
    "            all_perm = list(set(all_perm))\n",
    "            return all_perm\n",
    "        \n",
    "    def permute_consonants(self, word):\n",
    "        vowel_word = \"\".join([i if i in \"aeiou\" else \"_\" for i in word])\n",
    "        vowel_idxs = [i for i, letter in enumerate(vowel_word) if letter != \"_\"]\n",
    "        abridged_vowel_word = vowel_word.replace(\"_\", \"\")\n",
    "        all_permute_consonants = self.permute_all(abridged_vowel_word, vowel_permutation_loop=True)\n",
    "        permuted_consonants = []\n",
    "        for permuted_word in all_permute_consonants:\n",
    "            a = [\"_\"] * len(word)\n",
    "            for idx, vowel in enumerate(permuted_word):\n",
    "                a[vowel_idxs[idx]] = vowel\n",
    "            permuted_consonants.append(\"\".join(a))\n",
    "        return permuted_consonants\n",
    "\n",
    "    def create_masked_dictionary(self, df_aug):\n",
    "        masked_dictionary = {}\n",
    "        for counter, word in enumerate(df_aug['word']):\n",
    "            all_masked_words_for_word = self.permute_all(word)\n",
    "            all_masked_words_for_word += self.permute_consonants(word)\n",
    "            masked_dictionary[word] = list(set(all_masked_words_for_word))\n",
    "            if counter % 10000 == 0:\n",
    "                print(f\"Iteration {counter} completed\")\n",
    "        return masked_dictionary\n",
    "\n",
    "    def get_vowel_prob(self, df_vowel, vowel):\n",
    "        try:\n",
    "            return df_vowel[0].apply(lambda p: vowel in p).value_counts(normalize=True).loc[True]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def get_vowel_prior(self, df_aug):\n",
    "        prior_json = {}\n",
    "        for word_len in range(df_aug['length'].max()):\n",
    "            prior_json[word_len + 1] = []\n",
    "            df_vowel = df_aug[df_aug['length'] == word_len]\n",
    "            for vowel in ['a', 'e', 'i', 'o', 'u']:\n",
    "                prior_json[word_len + 1].append(self.get_vowel_prob(df_vowel, vowel))\n",
    "            prior_json[word_len + 1] = pd.DataFrame([pd.Series(['a', 'e', 'i', 'o', 'u']), pd.Series(prior_json[word_len + 1])]).T.sort_values(by=1, ascending=False)\n",
    "        return prior_json\n",
    "\n",
    "    def save_vowel_prior(self, vowel_prior):\n",
    "        with open(\"prior_probabilities.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vowel_prior, f)\n",
    "\n",
    "    def encode_output(self, word):\n",
    "        char_mapping = self.get_char_mapping()\n",
    "        last_char = word[-1]\n",
    "        if last_char not in char_mapping:\n",
    "            raise ValueError(f\"Invalid character '{last_char}' in word\")\n",
    "        return char_mapping[last_char] - 1\n",
    "\n",
    "\n",
    "    def encode_input(self, word):\n",
    "        char_mapping = self.get_char_mapping()\n",
    "        embedding_len = 35\n",
    "        word = ''.join(char for char in word if char in char_mapping)  # Remove non-mapped characters\n",
    "        word_vector = [0] * embedding_len\n",
    "        for idx, letter in enumerate(word, start=embedding_len - len(word)):\n",
    "            word_vector[idx] = char_mapping[letter]\n",
    "        return word_vector\n",
    "\n",
    "\n",
    "    def encode_words(self, masked_dictionary):\n",
    "        input_data = []\n",
    "        target_data = []\n",
    "        for output_word, input_words in masked_dictionary.items():\n",
    "            output_vector = self.encode_output(output_word)\n",
    "            for input_word in input_words:\n",
    "                input_data.append(self.encode_input(input_word))\n",
    "                target_data.append(output_vector)\n",
    "        return input_data, target_data\n",
    "\n",
    "    def save_input_output_data(self, input_data, target_data):\n",
    "        with open(r'input_features.txt', 'w') as fp:\n",
    "            for item in input_data:\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "            print('Done')\n",
    "        with open(r'target_features.txt', 'w') as fp:\n",
    "            for item in target_data:\n",
    "                fp.write(\"%s\\n\" % item)\n",
    "            print('Done')\n",
    "\n",
    "    def convert_to_tensor(self, input_data, target_data):\n",
    "        # Ensure input_data and target_data have the same length\n",
    "        assert len(input_data) == len(target_data)\n",
    "\n",
    "        # Convert input_data and target_data to tensors\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target_data, dtype=torch.long)\n",
    "\n",
    "        # Reshape the tensors to have the same batch size\n",
    "        input_tensor = input_tensor.view(-1, 1)\n",
    "        target_tensor = target_tensor.view(-1, 1)\n",
    "\n",
    "        return input_tensor, target_tensor\n",
    "    \n",
    "    def get_char_mapping(self):\n",
    "        char_mapping = {char: i for i, char in enumerate(string.ascii_lowercase, 1)}\n",
    "        char_mapping[' '] = 0  # Add an entry for the space character\n",
    "        return char_mapping\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        class BiLSTMModel(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super(BiLSTMModel, self).__init__()\n",
    "                self.embedding = torch.nn.Embedding(27, 128)\n",
    "                self.lstm = torch.nn.LSTM(128, 256, bidirectional=True, batch_first=True)\n",
    "                self.linear = torch.nn.Linear(256 * 2, 27)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.embedding(x)\n",
    "                x, _ = self.lstm(x)\n",
    "                x = self.linear(x[:, -1, :])\n",
    "                return torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        model = BiLSTMModel()\n",
    "        return model.to(self.device)  # Move model to GPU if available\n",
    "    \n",
    "    def load_model(self, epoch):\n",
    "        model = self.build_model()\n",
    "        model.load_state_dict(torch.load(f\"model_checkpoint_{epoch}.pth\"))\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    def train_model(self, epochs=10, lr=0.01, batch_size=32):\n",
    "        dataset = TensorDataset(self.input_tensor, self.target_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for batch in dataloader:\n",
    "                encoded_words_batch, next_letters_batch = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(encoded_words_batch)\n",
    "                next_letters_batch = next_letters_batch.view(-1)\n",
    "                loss = criterion(output, next_letters_batch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "            torch.save(self.model.state_dict(), f\"model_checkpoint_{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "    def guess(self, word, tries_remains):\n",
    "        word = word.strip()  # Remove leading and trailing whitespace\n",
    "        max_vowel_guess_limit = 5\n",
    "        if tries_remains > 4 and len(self.guessed_letters) <= max_vowel_guess_limit:\n",
    "            return self.guess_vowel(word)\n",
    "        else:\n",
    "            return self.guess_bilstm(word)\n",
    "\n",
    "    def guess_vowel(self, word):\n",
    "        word_len = len(word)\n",
    "        prior_prob = self.vowel_prior.get(word_len)\n",
    "        if not prior_prob.empty:  # Check if the DataFrame is not empty\n",
    "            for vowel, prob in prior_prob:\n",
    "                if vowel not in self.guessed_letters:\n",
    "                    return vowel\n",
    "        return None\n",
    "    \n",
    "    def guess_bilstm(self, word):\n",
    "        encoded_word = torch.tensor([self.encode_input(word)], dtype=torch.long).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(encoded_word)\n",
    "            _, predicted = torch.max(output.cpu().data, 1)\n",
    "            predicted_letter = chr(predicted.item() + ord('a'))\n",
    "        \n",
    "        if predicted_letter in self.guessed_letters:\n",
    "            _, topk_predicted = torch.topk(output.data, len(self.guessed_letters) + 1)\n",
    "            for i in range(len(self.guessed_letters) + 1):\n",
    "                predicted_letter = chr(topk_predicted[0, i].item() + ord('a'))\n",
    "                if predicted_letter not in self.guessed_letters:\n",
    "                    break\n",
    "\n",
    "        return predicted_letter\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location, \"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\": practice})\n",
    "        if response.get('status') == \"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(f\"Successfully start a new game! Game ID: {game_id}. # of tries remaining: {tries_remains}. Word: {word}.\")\n",
    "            while tries_remains > 0:\n",
    "                guess_letter = self.guess(word, tries_remains)\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(f\"Guessing letter: {guess_letter}\")\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\": \"guess_letter\", \"game_id\": game_id, \"letter\": guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(f\"Sever response: {res}\")\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status == \"success\":\n",
    "                    if verbose:\n",
    "                        print(f\"Successfully finished game: {game_id}\")\n",
    "                    return True\n",
    "                elif status == \"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(f\"Failed game: {game_id}. Because of: {reason}\")\n",
    "                    return False\n",
    "                elif status == \"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status == \"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        if self.access_token:\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is being used.\n",
      "Iteration 0 completed\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m api \u001b[38;5;241m=\u001b[39m \u001b[43mHangmanAPI\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m549f556bc32c3166631310e172e2e5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mHangmanAPI.__init__\u001b[1;34m(self, access_token, session, timeout)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_data)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 215\u001b[0m, in \u001b[0;36mHangmanAPI.train_model\u001b[1;34m(self, epochs, lr, batch_size)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m--> 215\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\data\\dataset.py:203\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    204\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[0;32m    205\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "api = HangmanAPI(access_token=\"549f556bc32c3166631310e172e2e5\", timeout=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully start a new game! Game ID: 0a316d1e1c68. # of tries remaining: 6. Word: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .\n",
      "Guessing letter: e\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: y\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 5, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: a\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 4, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: k\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 3, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: x\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 2, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: o\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 1, 'word': '_ e _ _ _ _ _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: u\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'ongoing', 'tries_remains': 1, 'word': '_ e _ _ _ u _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Guessing letter: n\n",
      "Sever response: {'game_id': '0a316d1e1c68', 'status': 'failed', 'tries_remains': 0, 'word': '_ e _ _ _ u _ _ _ _ _ _ _ _ _ e _ '}\n",
      "Failed game: 0a316d1e1c68. Because of: # of tries exceeded!\n",
      "run 106 practice games out of an allotted 100,000. practice success rate so far = 0.113\n"
     ]
    }
   ],
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
